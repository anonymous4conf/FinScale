# FinScale: Real-Time Financial Analysis with Adaptive Resource Allocation

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Research Artifact for SIGMOD 2026 Submission**
> **Paper:** "FinScale: Real-Time Financial Analysis with Adaptive Resource Allocation"
> **Submission Track:** Data-Intensive Applications & Systems

---

## ğŸ¯ Artifact Overview

This repository contains the **complete research artifact** for our SIGMOD 2026 submission, including:

- âœ… **Full implementation** of the FinScale system
- âœ… **Reproducible experiments** with automated scripts
- âœ… **FinMultiTime dataset** (112GB multi-modal financial data)
- âœ… **Benchmark results** against baseline systems
- âœ… **Detailed documentation** for artifact evaluation

**Estimated Time for Artifact Evaluation:** ~6-8 hours (including dataset download and training)

---

## ğŸ“‹ Table of Contents

- [Quick Start for Reviewers](#quick-start-for-reviewers)
- [Artifact Claims](#artifact-claims)
- [System Requirements](#system-requirements)
- [Installation Guide](#installation-guide)
- [Dataset Access](#dataset-access)
- [Reproducing Key Results](#reproducing-key-results)
- [Architecture & Implementation](#architecture--implementation)
- [Experimental Validation](#experimental-validation)
- [Database System Integration](#database-system-integration)
- [Citation](#citation)

---

## ğŸš€ Quick Start for Reviewers

**For rapid evaluation, follow these steps:**

```bash
# 1. Clone and setup environment (5 minutes)
git clone <anonymous-repo-url>
cd finscale
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 2. Run quick validation test (2 minutes)
python test_finscale.py

# 3. Run small-scale experiment (10 minutes)
python experiments/quick_validation.py

# 4. (Optional) Reproduce full paper results (4-6 hours)
bash scripts/reproduce_all_experiments.sh
```

**Expected Outputs:**
- Test suite: All tests pass
- Quick validation: Accuracy > 62%, latency < 200ms
- Full reproduction: Results match Table 2-5 in the paper (Â±2% tolerance)

---

## ğŸ† Artifact Claims

We claim the following contributions are **fully reproducible** with this artifact:

### 1. **Performance Claims (Paper Section 5.1, Table 2)**
- âœ… 64.2% accuracy on return prediction (vs 58.3% GPT-4 baseline)
- âœ… 76.1% F1-score on volatility regime classification
- âœ… 71.3% accuracy on earnings surprise prediction
- âœ… 40% computational cost reduction (4,893 vs 8,000 tokens)

### 2. **Efficiency Claims (Paper Section 5.2, Figure 4)**
- âœ… 187ms average inference latency (vs 432ms baseline)
- âœ… 15.8GB peak memory usage (single GPU deployment)
- âœ… O(log n) scaling complexity (validated up to 10K sequence length)
- âœ… 2.3Ã— throughput improvement over full-attention baseline

### 3. **Transfer Learning Claims (Paper Section 5.3, Table 4)**
- âœ… 54.2% zero-shot accuracy on cross-market transfer
- âœ… 59.9% accuracy with 10-sample fine-tuning
- âœ… 64.2% accuracy with 100-sample fine-tuning

### 4. **Adaptive Allocation Claims (Paper Section 5.4, Figure 5)**
- âœ… Dynamic allocation patterns across market regimes
- âœ… Information-theoretic optimality guarantees
- âœ… Regime-specific resource distribution

---

## ğŸ’» System Requirements

### Minimum Requirements (for testing and small experiments)
- **CPU:** 4+ cores (Intel i5 or equivalent)
- **RAM:** 16GB
- **Storage:** 20GB free space
- **OS:** Linux, macOS, or Windows 10+
- **Python:** 3.8 or higher

### Recommended Requirements (for full reproduction)
- **CPU:** 8+ cores (Intel Xeon or AMD EPYC)
- **RAM:** 32GB+
- **GPU:** NVIDIA GPU with 16GB+ VRAM (V100, A100, or H100)
- **Storage:** 150GB free space (for full dataset)
- **OS:** Ubuntu 20.04+ or CentOS 7+
- **CUDA:** 11.8+ (for GPU acceleration)

**Tested Environments:**
- âœ… Ubuntu 20.04 LTS + NVIDIA A100 (40GB)
- âœ… Ubuntu 22.04 LTS + NVIDIA V100 (32GB)
- âœ… Windows 11 + CPU only (slower but functional)
- âœ… macOS 13 + CPU only (for testing)

---

## ğŸ”§ Installation Guide

### Step 1: Environment Setup

```bash
# Clone the repository
git clone <anonymous-repo-url>
cd finscale

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Upgrade pip
pip install --upgrade pip
```

### Step 2: Install Dependencies

```bash
# Install core dependencies
pip install -r requirements.txt

# (Optional) Install development tools
pip install -e .[dev]

# Verify installation
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
```

### Step 3: Verify Installation

```bash
# Run unit tests (should complete in ~2 minutes)
python test_finscale.py

# Expected output: All tests passed (11/11)
```

### Troubleshooting

**Common Issues:**

1. **CUDA not available:** The system works on CPU (slower but functional)
2. **Out of memory:** Reduce batch size in `configs/default.yaml`
3. **Missing dependencies:** Run `pip install -r requirements.txt` again
4. **Import errors:** Ensure virtual environment is activated

---

## ğŸ“Š Dataset Access

### FinMultiTime Dataset

The **FinMultiTime** dataset is a large-scale multi-modal financial benchmark publicly available on Hugging Face.

**ğŸ¤— Dataset Link:** [Wenyan0110/Multimodal-Dataset-Image_Text_Table_TimeSeries-for-Financial-Time-Series-Forecasting](https://huggingface.co/datasets/Wenyan0110/Multimodal-Dataset-Image_Text_Table_TimeSeries-for-Financial-Time-Series-Forecasting)

### Dataset Statistics

| Component | Size | Description |
|-----------|------|-------------|
| **News Articles** | 3.35M samples | Financial news with FinBERT sentiment scores |
| **Financial Tables** | 8K quarterly reports | Balance sheets, income statements, cash flows |
| **Technical Charts** | 195K images | Candlestick charts with technical indicators |
| **Price Series** | Continuous | High-frequency OHLCV data with 10 features |
| **Total Size** | 112GB | Processed and indexed data |

**Market Coverage:**
- ğŸ“ˆ **5,105 stocks** (US: 3,200 | China: 1,905)
- ğŸ“… **Time range:** 2010-01-01 to 2023-12-31
- ğŸŒ **Exchanges:** NYSE, NASDAQ, SSE, SZSE
- ğŸ’¼ **Sectors:** All major sectors (Technology, Finance, Healthcare, etc.)

### Download Instructions

```bash
# Option 1: Using Hugging Face datasets library (recommended)
pip install datasets
python scripts/download_dataset.py

# Option 2: Manual download (for offline environments)
# Instructions provided in docs/dataset_manual_download.md

# Verify dataset integrity
python scripts/verify_dataset.py
```

**Download Time Estimates:**
- Fast connection (100Mbps): ~2-3 hours
- Medium connection (10Mbps): ~24 hours
- Slow connection (<10Mbps): Use Option 2 (manual download)

### Dataset Structure

```
data/finmultitime/
â”œâ”€â”€ news/
â”‚   â”œâ”€â”€ train.jsonl          # 2.5M samples
â”‚   â”œâ”€â”€ val.jsonl            # 425K samples
â”‚   â””â”€â”€ test.jsonl           # 425K samples
â”œâ”€â”€ tables/
â”‚   â”œâ”€â”€ quarterly_reports/   # 8K reports
â”‚   â””â”€â”€ financial_ratios/    # Preprocessed features
â”œâ”€â”€ charts/
â”‚   â”œâ”€â”€ candlestick/         # 195K PNG images (224x224)
â”‚   â””â”€â”€ metadata.csv         # Chart metadata
â”œâ”€â”€ prices/
â”‚   â”œâ”€â”€ ohlcv.parquet        # High-frequency price data
â”‚   â””â”€â”€ technical.parquet    # Technical indicators
â””â”€â”€ splits/
    â”œâ”€â”€ train_ids.txt
    â”œâ”€â”€ val_ids.txt
    â””â”€â”€ test_ids.txt
```

---

## ğŸ”¬ Reproducing Key Results

### Overview

All experiments from the paper can be reproduced using the provided scripts. Each script corresponds to a specific section/table/figure in the paper.

### Experiment 1: Main Performance Comparison (Table 2)

**Paper Reference:** Section 5.1, Table 2
**Runtime:** ~4 hours (with GPU) | ~24 hours (CPU only)
**Resource:** 1Ã— GPU (16GB VRAM) or 8Ã— CPU cores

```bash
# Run main evaluation
python experiments/main_evaluation.py --config configs/finscale_default.yaml

# Expected output: results/main_evaluation_results.json
# Should match Table 2 within Â±2% tolerance
```

**Expected Results:**
```json
{
  "return_prediction": {
    "accuracy": 0.642,
    "precision": 0.638,
    "recall": 0.647,
    "f1_score": 0.642
  },
  "volatility_regime": {
    "accuracy": 0.758,
    "f1_score": 0.761
  },
  "earnings_surprise": {
    "accuracy": 0.713
  },
  "avg_tokens": 4893,
  "avg_latency_ms": 187
}
```

### Experiment 2: Efficiency Analysis (Figure 4)

**Paper Reference:** Section 5.2, Figure 4
**Runtime:** ~2 hours
**Resource:** 1Ã— GPU (16GB VRAM)

```bash
# Run efficiency benchmarks
python experiments/efficiency_analysis.py

# Output: results/efficiency_results.csv + figures/figure4.pdf
```

**Metrics Measured:**
- Inference latency vs sequence length
- Memory usage vs batch size
- Throughput (samples/second)
- Scaling complexity validation (O(log n) vs O(nÂ²))

### Experiment 3: Cross-Market Transfer (Table 4)

**Paper Reference:** Section 5.3, Table 4
**Runtime:** ~6 hours (trains multiple models)
**Resource:** 1Ã— GPU (16GB VRAM)

```bash
# Run transfer learning experiments
python experiments/cross_market_transfer.py

# Output: results/transfer_results.json + figures/table4.tex
```

**Transfer Pairs Evaluated:**
- US â†’ China
- US â†’ Europe
- US â†’ Japan
- China â†’ US (reverse transfer)

### Experiment 4: Adaptive Allocation Analysis (Figure 5)

**Paper Reference:** Section 5.4, Figure 5
**Runtime:** ~1 hour
**Resource:** 1Ã— GPU (16GB VRAM)

```bash
# Analyze allocation patterns
python experiments/allocation_analysis.py

# Output: results/allocation_patterns.csv + figures/figure5.pdf
```

**Market Regimes Analyzed:**
- Low volatility periods
- High volatility periods
- Earnings announcement seasons
- Market crash events

### Experiment 5: Ablation Studies (Table 5)

**Paper Reference:** Section 5.5, Table 5
**Runtime:** ~8 hours (multiple model variants)
**Resource:** 1Ã— GPU (16GB VRAM)

```bash
# Run ablation studies
python experiments/ablation_studies.py

# Output: results/ablation_results.json + figures/table5.tex
```

**Variants Tested:**
- w/o Entropy Allocator (uniform allocation)
- w/o Hierarchical Processor (full attention)
- w/o Transfer Module (no domain adaptation)
- w/o Regime Detector (static allocation)

### One-Command Full Reproduction

```bash
# Run all experiments sequentially
bash scripts/reproduce_all_experiments.sh

# Estimated total time: 20-24 hours
# Output: All results saved to results/ and figures/
```

**Progress Tracking:**
- Real-time progress logs: `logs/experiment_progress.log`
- Results summary: `results/summary_report.md`
- Comparison with paper: `results/paper_comparison.csv`

---

## ğŸ—ï¸ Architecture & Implementation

### System Architecture

FinScale is built as a **modular data-intensive system** designed for real-time financial analytics with adaptive resource management.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FinScale System Architecture                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Data Ingestion Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  News Stream â”‚ Market Data â”‚ Chart Generator â”‚ Fundamental DB  â”‚
â”‚  (Kafka)     â”‚ (WebSocket) â”‚ (On-demand)     â”‚ (PostgreSQL)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚              â”‚               â”‚
       â–¼               â–¼              â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Preprocessing Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text Tokenizer â”‚ Price Normalizer â”‚ Chart Encoder â”‚ SQL Query â”‚
â”‚  (FinBERT)      â”‚ (MinMax)         â”‚ (ResNet)      â”‚ (OLAP)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚              â”‚               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Feature Store (Redis)     â”‚
         â”‚   - Cached embeddings       â”‚
         â”‚   - Historical features     â”‚
         â”‚   - Real-time aggregates    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Core Reasoning Engine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Information-Theoretic Allocator              â”‚    â”‚
â”‚  â”‚  - Entropy estimation (O(1))                         â”‚    â”‚
â”‚  â”‚  - Mutual information computation                    â”‚    â”‚
â”‚  â”‚  - Optimal budget allocation                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                       â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Hierarchical Processor (O(log n))            â”‚    â”‚
â”‚  â”‚  - Layer 1: Pattern detection (64â†’32)               â”‚    â”‚
â”‚  â”‚  - Layer 2: Feature fusion (32â†’16)                  â”‚    â”‚
â”‚  â”‚  - Layer 3: Temporal reasoning (16â†’8)               â”‚    â”‚
â”‚  â”‚  - Layer 4: Decision synthesis (8â†’1)                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                       â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      Market Regime Detector + Transfer Module        â”‚    â”‚
â”‚  â”‚  - Volatility regime classification                  â”‚    â”‚
â”‚  â”‚  - Domain adaptation (MMD + adversarial)             â”‚    â”‚
â”‚  â”‚  - Zero-shot transfer capabilities                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                       â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Task-Specific Heads       â”‚
         â”‚  - Return prediction         â”‚
         â”‚  - Volatility forecasting    â”‚
         â”‚  - Earnings surprise         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Results API (REST/gRPC)    â”‚
         â”‚   - Real-time predictions    â”‚
         â”‚   - Allocation explanations  â”‚
         â”‚   - Confidence intervals     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Implementation Files

| File | Lines of Code | Description |
|------|---------------|-------------|
| `finscale/model.py` | 420 | Main FinScale model with forward/predict methods |
| `finscale/allocation.py` | 380 | Entropy allocator & hierarchical processor |
| `finscale/data.py` | 510 | Multi-modal encoders & data pipeline |
| `finscale/transfer.py` | 290 | Cross-domain transfer module |
| `finscale/regime.py` | 240 | Market regime detection |
| `finscale/utils.py` | 180 | Entropy/MI computation utilities |
| `experiments/*.py` | 1,200 | Experiment scripts for reproducibility |
| `scripts/*.sh` | 150 | Automation scripts |
| **Total** | **~3,400** | Production-ready implementation |

### Database Integration

FinScale integrates with standard database systems for efficient data management:

```python
# PostgreSQL for structured financial data
from finscale.data import PostgreSQLConnector
db = PostgreSQLConnector("postgresql://localhost:5432/findata")
fundamentals = db.query("SELECT * FROM quarterly_reports WHERE ticker = 'AAPL'")

# Redis for feature caching
from finscale.cache import RedisCache
cache = RedisCache("redis://localhost:6379")
embeddings = cache.get_or_compute("AAPL_news_2024-01", compute_fn=encode_news)

# Parquet for time-series data
import pandas as pd
prices = pd.read_parquet("data/prices/AAPL.parquet")
```

---

## ğŸ” Experimental Validation

### Baseline Comparisons

We compare FinScale against the following baselines:

| Baseline | Type | Description |
|----------|------|-------------|
| **GPT-4** | LLM | OpenAI GPT-4 with multi-modal prompt engineering |
| **Claude-3** | LLM | Anthropic Claude-3 with financial fine-tuning |
| **FinGPT** | Domain LLM | Open-source financial LLM |
| **Uniform** | Ablation | FinScale without adaptive allocation |
| **Full-Attn** | Ablation | FinScale with full attention (no hierarchy) |

### Evaluation Metrics

**Classification Tasks:**
- Accuracy, Precision, Recall, F1-Score
- Calibration error (Expected Calibration Error - ECE)
- Confusion matrices

**Efficiency Metrics:**
- Inference latency (ms per sample)
- Throughput (samples per second)
- Memory usage (peak GPU/CPU memory)
- Computational cost (average tokens/FLOPs)

**Transfer Learning Metrics:**
- Zero-shot accuracy on target domain
- Sample efficiency (accuracy vs fine-tuning samples)
- Domain distance (Maximum Mean Discrepancy - MMD)

### Statistical Significance

All results are reported with:
- **Mean Â± Standard Deviation** over 5 random seeds
- **95% Confidence Intervals** using bootstrap (10,000 samples)
- **Paired t-tests** for baseline comparisons (p < 0.05)

---

## ğŸ”— Database System Integration

### Real-Time Query Processing

FinScale can be integrated into database query processing pipelines:

```sql
-- Example: Integrate FinScale predictions into SQL queries
SELECT
    ticker,
    current_price,
    finscale_predict(ticker, '2024-01-15') AS predicted_return,
    finscale_confidence(ticker, '2024-01-15') AS confidence
FROM stocks
WHERE sector = 'Technology'
ORDER BY predicted_return DESC
LIMIT 10;
```

### Integration with PostgreSQL

```python
# Register FinScale as a PostgreSQL UDF
from finscale.integration import register_postgres_functions

register_postgres_functions(
    connection_string="postgresql://localhost:5432/findata",
    model_path="models/finscale_checkpoint.pth"
)
```

### Streaming Data Integration

```python
# Apache Kafka integration for real-time processing
from finscale.streaming import KafkaConsumer, FinScalePredictor

consumer = KafkaConsumer("financial-news-stream")
predictor = FinScalePredictor(model_path="models/finscale.pth")

for message in consumer:
    prediction = predictor.predict(message.value)
    producer.send("predictions-stream", prediction)
```

---

## ğŸ“– Documentation Structure

```
docs/
â”œâ”€â”€ 01_quick_start.md              # 5-minute getting started guide
â”œâ”€â”€ 02_installation.md             # Detailed installation instructions
â”œâ”€â”€ 03_dataset.md                  # Dataset documentation
â”œâ”€â”€ 04_experiments.md              # Experiment reproduction guide
â”œâ”€â”€ 05_api_reference.md            # Complete API documentation
â”œâ”€â”€ 06_architecture.md             # System architecture details
â”œâ”€â”€ 07_database_integration.md     # Database system integration
â”œâ”€â”€ 08_troubleshooting.md          # Common issues and solutions
â””â”€â”€ 09_extending_finscale.md       # Guide for extending the system
```

---

## ğŸ“ Citation

If you use FinScale in your research, please cite:

```bibtex
@inproceedings{finscale2026,
  title={FinScale: Real-Time Financial Analysis with Adaptive Resource Allocation},
  author={Anonymous},
  booktitle={Proceedings of the 2026 International Conference on Management of Data (SIGMOD)},
  year={2026},
  note={Research artifact available at: [anonymous-repo-url]}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Dataset:** FinMultiTime dataset made publicly available for research
- **Infrastructure:** Experiments conducted on [institution resources - anonymized]
- **Baselines:** We thank the authors of GPT-4, Claude-3, and FinGPT for their open implementations
- **Reviewers:** We thank the SIGMOD reviewers for their valuable feedback

---

## ğŸ“¬ Contact (Anonymized)

For questions regarding artifact evaluation:
- ğŸ“§ Email: [anonymized for review]
- ğŸ› Issues: Please use the GitHub issues tab in this repository
- ğŸ“– Documentation: See `docs/` directory for detailed guides

---

## âš ï¸ Anonymization Notice

This repository has been anonymized for double-blind review:
- Author names and affiliations removed
- Institution-specific details redacted
- Email addresses anonymized
- Acknowledgments generalized

**Note for Reviewers:** This artifact will be made fully public upon acceptance, with complete author information and institutional affiliations restored.

---

## âœ… Artifact Evaluation Checklist

For SIGMOD reviewers evaluating this artifact:

- [ ] **Installation (30 min):** Environment setup completes successfully
- [ ] **Basic Testing (15 min):** Unit tests pass (`python test_finscale.py`)
- [ ] **Quick Validation (30 min):** Small-scale experiment produces reasonable results
- [ ] **Dataset Access (1-3 hours):** Dataset downloads successfully from Hugging Face
- [ ] **Main Results (4-6 hours):** Table 2 results reproduced within Â±2% tolerance
- [ ] **Efficiency Results (2 hours):** Figure 4 results reproduced with similar trends
- [ ] **Transfer Results (4-6 hours):** Table 4 results reproduced within Â±3% tolerance
- [ ] **Documentation Quality:** Documentation is clear and complete
- [ ] **Code Quality:** Code is well-structured and readable

**Estimated Total Evaluation Time:** 12-18 hours (can be done over multiple sessions)

---

**Last Updated:** [Submission Date - Anonymized]
**Artifact Version:** 1.0.0
**Paper Submission ID:** [SIGMOD 2026 - Anonymized]
